import json
import pandas as pd
from datetime import datetime

def extract_data(json_file_path):
    """
    Extract data from the JSON file
    """
    try:
        with open(json_file_path, 'r', encoding='utf-8-sig') as file:
            data = json.load(file)
        print(f"Successfully extracted {len(data['Locations'])} locations")
        return data
    except Exception as e:
        print(f"Error reading file: {e}")
        return None

def transform_to_dataframe(data):
    """
    Transform JSON data to complete flattened DataFrame
    """
    print("Flattening data to DataFrame...")
    
    flattened_data_complete = []
    
    # Loop through all locations -> evses -> connectors
    for location in data['Locations']:
        for evse in location['evses']:
            for connector in evse['connectors']:
                # Create complete row with all fields in one command
                row = {
                    # Location-level fields
                    'location_country_code': location.get('country_code', ''),
                    'location_party_id': location.get('party_id', ''),
                    'location_id': location.get('id', ''),
                    'location_publish': location.get('publish', False),
                    'location_name': location.get('name', ''),
                    'location_address': location.get('address', ''),
                    'location_city': location.get('city', ''),
                    'location_postal_code': location.get('postal_code', ''),
                    'location_state': location.get('state', ''),
                    'location_country': location.get('country', ''),
                    'location_coordinates_latitude': location.get('coordinates', {}).get('latitude', ''),
                    'location_coordinates_longitude': location.get('coordinates', {}).get('longitude', ''),
                    'location_parking_type': location.get('parking_type', ''),
                    'location_operator_name': location.get('operator', {}).get('name', ''),
                    'location_owner_name': location.get('owner', {}).get('name', ''),
                    'location_opening_times_twentyfourseven': location.get('opening_times', {}).get('twentyfourseven', False),
                    'location_time_zone': location.get('time_zone', ''),
                    'location_last_updated': location.get('last_updated', ''),
                     # EVSE-level fields
                    'evse_uid': evse.get('uid', ''),
                    'evse_id': evse.get('evse_id', ''),
                    'evse_status': evse.get('status', ''),
                    'evse_coordinates_latitude': evse.get('coordinates', {}).get('latitude', ''),
                    'evse_coordinates_longitude': evse.get('coordinates', {}).get('longitude', ''),
                    'evse_capabilities': str(evse.get('capabilities', [])),
                    'evse_floor_level': evse.get('floor_level', ''),
                    'evse_supports_roaming': evse.get('supports_roaming', False),
                    'evse_manufacturer': evse.get('manufacturer', ''),
                    'evse_last_updated': evse.get('last_updated', ''),
                    # Connector-level fields
                    'connector_id': connector.get('id', ''),
                    'connector_standard': connector.get('standard', ''),
                    'connector_format': connector.get('format', ''),
                    'connector_power_type': connector.get('power_type', ''),
                    'connector_max_voltage': connector.get('max_voltage', 0),
                    'connector_max_amperage': connector.get('max_amperage', 0),
                    'connector_max_electric_power': connector.get('max_electric_power', 0),
                    'connector_last_updated': connector.get('last_updated', '')
                }
                
                flattened_data_complete.append(row)
    
    # Convert to DataFrame
    df = pd.DataFrame(flattened_data_complete)
    print(f"Created DataFrame with shape: {df.shape}")
    
    return df

def sanity_checks(raw_data, df, etl_result):
    """
    Perform sanity checks on data structure and results validation
    """
    print("\nPerforming sanity checks...")
    
    # 1. DATA STRUCTURE CHECKS
    print("\nData Structure Validation:")
    
    # Check raw JSON structure
    print(f"   JSON top-level keys: {list(raw_data.keys())}")
    print(f"   Total locations in JSON: {len(raw_data['Locations'])}")
    
    # Check first location structure
    if raw_data['Locations']:
        first_location = raw_data['Locations'][0]
        print(f"   Location has {len(first_location.keys())} fields")
        print(f"   First location party_id: {first_location.get('party_id', 'MISSING')}")
        
        # Check EVSE structure
        if first_location.get('evses'):
            first_evse = first_location['evses'][0]
            print(f"   EVSE has {len(first_evse.keys())} fields")
            
            # Check connector structure
            if first_evse.get('connectors'):
                first_connector = first_evse['connectors'][0]
                print(f"   Connector has {len(first_connector.keys())} fields")
                print(f"   First connector power_type: {first_connector.get('power_type', 'MISSING')}")
    
    # Check DataFrame structure
    print(f"\nDataFrame Validation:")
    print(f"   DataFrame shape: {df.shape}")
    print(f"   DataFrame columns: {len(df.columns)}")
    print(f"   Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
    
    # Check for missing critical fields
    critical_fields = ['location_party_id', 'location_id', 'connector_power_type']
    for field in critical_fields:
        missing_count = df[field].isna().sum()
        print(f"   Missing {field}: {missing_count}")
    
    # 2. RESULTS VALIDATION
    print(f"\nResults Validation:")
    
    # Cross-check party IDs count
    manual_party_count = len(set(location['party_id'] for location in raw_data['Locations'] if location.get('party_id')))
    df_party_count = etl_result['party_ids_count']
    print(f"   Party IDs - Manual count: {manual_party_count}, DataFrame count: {df_party_count}")
    if manual_party_count == df_party_count:
        print("     Party IDs count MATCHES")
    else:
        print("     Party IDs count MISMATCH!")
    
    # Cross-check total locations
    total_locations_json = len(raw_data['Locations'])
    total_locations_calc = sum(item['locations'] for item in etl_result['total_locations_per_party_id'])
    print(f"   Total locations - JSON: {total_locations_json}, Calculated: {total_locations_calc}")
    if total_locations_json == total_locations_calc:
        print("     Total locations count MATCHES")
    else:
        print("     Total locations count MISMATCH!")
    
    # Cross-check power types
    manual_power_types = set()
    for location in raw_data['Locations']:
        for evse in location.get('evses', []):
            for connector in evse.get('connectors', []):
                if connector.get('power_type'):
                    manual_power_types.add(connector['power_type'])
    
    manual_power_count = len(manual_power_types)
    df_power_count = len(etl_result['unique_power_types'])
    print(f"   Power types - Manual count: {manual_power_count}, DataFrame count: {df_power_count}")
    if manual_power_count == df_power_count:
        print("     Power types count MATCHES")
    else:
        print("     Power types count MISMATCH!")
    
    # Show sample data quality
    print(f"\nData Quality Summary:")
    print(f"   Total connectors in DataFrame: {len(df)}")
    print(f"   Unique locations: {df['location_id'].nunique()}")
    print(f"   Unique EVSEs: {df['evse_id'].nunique()}")
    print(f"   Unique connectors: {df['connector_id'].nunique()}")
    
    print("\nSanity checks completed")

def calculate_etl_metrics(df):
    """
    Calculate the required ETL metrics from DataFrame
    """
    print("Calculating ETL metrics...")
    
    # 1. Count of unique Party IDs
    party_ids_count = df['location_party_id'].nunique()
    print(f"   - Unique Party IDs: {party_ids_count}")
    
    # 2. Total locations per Party ID
    locations_per_party = df.groupby('location_party_id')['location_id'].nunique().reset_index()
    locations_per_party.columns = ['party_id', 'locations']
    
    # Convert to list of dictionaries for JSON output
    total_locations_per_party_id = locations_per_party.to_dict('records')
    print(f"   - Party-location combinations: {len(total_locations_per_party_id)}")
    
    # 3. Unique Power Types
    unique_power_types = sorted(df['connector_power_type'].unique().tolist())
    print(f"   - Unique Power Types: {len(unique_power_types)}")
    
    # 4. Retrieved timestamp (using current time for now)
    retrieved_at = datetime.now().strftime("%d.%m.%Y - %H:%M:%S")
    
    # Create final result structure
    etl_result = {
        "party_ids_count": party_ids_count,
        "total_locations_per_party_id": total_locations_per_party_id,
        "unique_power_types": unique_power_types,
        "retrieved_at": retrieved_at
    }
    
    return etl_result

def load_data(etl_result, output_file):
    """
    Load transformed data into JSON file
    """
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(etl_result, f, indent=2, ensure_ascii=False)
        print(f"Results saved to: {output_file}")
        return True
    except Exception as e:
        print(f"Error saving file: {e}")
        return False

def main():
    """
    Complete ETL pipeline using DataFrame approach
    """
    # File paths
    input_file = r"C:\Users\lambros.vagias\Desktop\Bau\Learning\GR.IDRO.static.data.latest.json\GR.IDRO.static.data.latest.json"
    output_file = 'etl_dataframe_output.json'
    
    print("Starting DataFrame-based ETL Pipeline...")
    
    # Extract
    print("\nEXTRACT")
    raw_data = extract_data(input_file)
    if not raw_data:
        print("Failed to extract data. Exiting.")
        return
    
    # Transform
    print("\nTRANSFORM")
    df_complete = transform_to_dataframe(raw_data)
    etl_result = calculate_etl_metrics(df_complete)
    
    # Sanity Checks
    sanity_checks(raw_data, df_complete, etl_result)
    
    # Load
    print("\nLOAD")
    success = load_data(etl_result, output_file)
    
    if success:
        print(f"\nETL Pipeline completed successfully!")
        print("\nFINAL RESULTS:")
        print(f"   Party IDs count: {etl_result['party_ids_count']}")
        print(f"   Total locations: {sum(item['locations'] for item in etl_result['total_locations_per_party_id'])}")
        print(f"   Unique power types: {len(etl_result['unique_power_types'])}")
        print(f"   Retrieved at: {etl_result['retrieved_at']}")
        print(f"   Output file: {output_file}")
    else:
        print("ETL pipeline failed during loading.")

if __name__ == "__main__":
    main()
